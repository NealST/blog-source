---
title: 为什么说“下一个 Token 预测”是对 AI 认知的陈腐偏见
date: 2026-01-31 18:00:00
tags: "AI"
---

![](https://youke.xn--y7xa690gmna.cn/s1/2026/01/31/697dc8c103114.webp)[^fig1]
[^fig1]: “一张鹦鹉的电影级特写，它半张脸显露为先进的控制论机械结构，象征着 AI 推理背后隐藏的复杂性。” — 由 Nano Banana 生成

> 在 medium 上看到了博主 Adham Khaled 对于 AI 认知的一个分享，觉得非常不错。为了方便大家理解，我做了结构梳理，行文优化和内容补充。

## 还原论的陷阱：被误解的点火装置

在 AI 圈的讨论以及日常工作中，我们经常常能听到一种深度还原论的观点：“LLM 本质上就是根据概率预测下一个 Token，这模型没有灵魂，只是一只‘随机鹦鹉’罢了”。

从物理层面看，这没什么毛病，就像我们可以说：
* 法拉利引擎的本质，不过是一连串受控的汽油爆炸；
* 人类大脑的本质，不过是碳基神经元之间的电化学信号交换。

**但技术分析不能止步于“点火装置”。** 如果你只盯着爆炸，那就没法解释赛车为何能以 300 公里的时速过弯；如果只盯着 Token 预测，那你将完全错过 AI 推理时代的新质生产力。

AI 已经从文本概率进化到了策略优化。支撑这一跨越的，是底层算法逻辑的全面迭代：从早期的 RLHF（基于人类反馈的强化学习），再到后来的 DPO（直接偏好优化），以及如今的 GRPO（群体相对策略优化） 与 RLVR（基于可验证奖励的强化学习）。

## 1.0 时代：强化学习与讨好型人格的局限

在 GPT-3 时代（2020–2022），我们主要依靠 **RLHF（基于人类反馈的强化学习）**。

当时的逻辑类似训狗：模型生成答案，人类给出好评或差评。为了量化这个过程，工程师们引入了 **PPO（近端策略优化）** 算法，通过一个额外的奖励模型来充当评委。

*   **缺陷：** 这一阶段的模型是讨好型人格。它们学习的是如何让答案看起来像人类喜欢的样子，而非什么是正确的逻辑，这就带来了严重的幻觉问题 — 模型会为了显得专业而编造事实。
*   **结论：** 此时的 AI 确实更像一只学舌的鹦鹉，只是模仿人类的语调，而非理解世界的真理。

## 2.0 时代：DPO 与 偏好的内生化

到了2024年，**DPO（直接偏好优化）** 逐渐取代了复杂的 PPO。

DPO 取消了中间环节的“奖励模型”，直接将人类的偏好数据喂给模型，比如：

* 答案 A： “中国的首都名为北京。”（胜者）
* 答案 B： “中国的首都是一种烤鸭。”（败者）

DPO 的策略是将这些数据直接喂进模型的损失函数中，告诉算法：最大化 A 的概率，最小化 B 的概率。

虽然 DPO 极大提升了训练效率，并让 AI 变得更聪明、更符合人类价值观，但它依然建立在**模仿偏好**的基础上，直到推理模型的爆发，AI 才真正进化到了了逻辑推理的阶段。

## 推理时代：GRPO 如何重塑 AI 的思考

在 2024 年末和 2025 年初，DeepSeek-R1 等模型的崛起，标志着 **GRPO（群体相对策略优化）** 时代的到来。

GRPO 不再需要人类手把手教模型怎么写，而是让模型在实战中自我进化：
1.  **群体竞争：** 面对一道复杂数学题，模型同时生成 16 个甚至更多解题路径。
2.  **自我博弈：** 模型不依赖外部评分，而是通过群体内部的相对比较，识别出哪条路径导致了成功，哪条导致了死胡同。
3.  **强化路径：** 那些通往正确答案的神经通路被反复强化，错误的被抑制。

这就已经超越了概率预测的范畴，进入了策略博弈的领域。模型在学习如何调度其有限的计算资源（思维链），以达成逻辑上的自洽。它不仅只是预测下一个Token，而是在构建一个通往真理的路径图。

## 为什么 AI 写代码比你强：RLVR 带来的硬约束

如果说 GRPO 是让 AI 自我思考，那么 **RLVR（基于可验证奖励的强化学习）** 则是给 AI 叠上了一套“客观真理”的 BUFF。

在编程、数学、逻辑证明等领域，好坏不再由人类主观判定，而是由编译器和逻辑判定引擎决定：
* 代码写错了？ 运行一下，报错即惩罚。
* 数学算错了？验证结果，错误即归零。

在 RLVR 的反馈循环中，AI 不再关心人类想听什么，它只关心物理世界中什么是运行得通的。这种基于Ground Truth（基本真理）的进化，让 AI 拥有了超越人类经验的自学习能力。

## 结语：认知落后是最大的风险

当一些人还在争论 AI 是否有“意识”、是否只是“统计学”时，部分领先的工程师已经开始利用这些推理引擎重构代码库、加速新药研发、推导物理公式。

**下一个 Token 预测这种论调只是 AI 的物理外壳，而目标驱动的策略优化才是它的灵魂。**

认知指导行动，如果你相信 AI 只是只鹦鹉，你就只会像用鹦鹉一样使用它，让它写写邮件、总结一下会议纪要，把它当成玩具。

在这个“推理时代”，我们需要放下这些过时的偏见。**AI 正在学会思考，而我们首先需要学会如何正确看待它的思考。**
